{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import math\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "\n",
    "# utils\n",
    "sys.path.insert(0, '/tf/notebooks/other/kaggle')\n",
    "from utils import utils_featexp\n",
    "from utils import utils_features_engineering\n",
    "from utils import utils_features_plots\n",
    "from utils import utils_reduce_memory\n",
    "from utils import utils_statistic\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train transaction data:590540\n",
      "Number of train identity data:144233\n",
      "Number of test transaction data:506691\n",
      "Number of test identity data:141907\n"
     ]
    }
   ],
   "source": [
    "START_DATE = datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "main_path = Path('../../input/ieee-cis-fraud-detection/')\n",
    "\n",
    "train_transaction_data = pd.read_csv(main_path / 'train_transaction.csv')\n",
    "train_identity_data = pd.read_csv(main_path / 'train_identity.csv')\n",
    "test_transaction_data = pd.read_csv(main_path / 'test_transaction.csv')\n",
    "test_identity_data = pd.read_csv(main_path / 'test_identity.csv')\n",
    "\n",
    "print('Number of train transaction data:{}\\nNumber of train identity data:{}'.format(\n",
    "    train_transaction_data.shape[0],\n",
    "    train_identity_data.shape[0]\n",
    "))\n",
    "\n",
    "print('Number of test transaction data:{}\\nNumber of test identity data:{}'.format(\n",
    "    test_transaction_data.shape[0],\n",
    "    test_identity_data.shape[0]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features selection - time consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import lightgbm as lgb\n",
    "\n",
    "START_DATE = datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "train_transaction_data['DT_M'] = train_transaction_data['TransactionDT'].apply(lambda x: (START_DATE + timedelta(seconds = x)))\n",
    "train_transaction_data['DT_M'] = (train_transaction_data['DT_M'].dt.year-2017)*12 + train_transaction_data['DT_M'].dt.month \n",
    "\n",
    "# SPLIT DATA INTO FIRST MONTH AND LAST MONTH\n",
    "train = train_transaction_data[train_transaction_data.DT_M==12].copy()\n",
    "validate = train_transaction_data[train_transaction_data.DT_M==17].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\ttrain's auc: 0.504236\tvalid's auc: 0.502194\n",
      "[100]\ttrain's auc: 0.504236\tvalid's auc: 0.502194\n",
      "[150]\ttrain's auc: 0.504236\tvalid's auc: 0.502194\n",
      "[200]\ttrain's auc: 0.504236\tvalid's auc: 0.502194\n",
      "[250]\ttrain's auc: 0.504236\tvalid's auc: 0.502194\n",
      "[300]\ttrain's auc: 0.504236\tvalid's auc: 0.502194\n",
      "[350]\ttrain's auc: 0.504236\tvalid's auc: 0.502194\n",
      "[400]\ttrain's auc: 0.504236\tvalid's auc: 0.502194\n",
      "[450]\ttrain's auc: 0.504236\tvalid's auc: 0.502194\n",
      "[500]\ttrain's auc: 0.504236\tvalid's auc: 0.502194\n",
      "Best auc score:\n",
      "train:0.5042\n",
      "valid:0.5022\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND VALIDATE\n",
    "col = 'C3'\n",
    "num_verbose_eval = 50\n",
    "lgbm = lgb.LGBMClassifier(n_estimators=500, objective='binary', num_leaves=8, learning_rate=0.02, metric='auc')\n",
    "h = lgbm.fit(\n",
    "    train[[col]], \n",
    "    train.isFraud,     \n",
    "    eval_metric='auc', \n",
    "    eval_set=[(train[[col]],train.isFraud),\n",
    "              (validate[[col]],validate.isFraud)],\n",
    "    eval_names=['train', 'valid'],\n",
    "    verbose=num_verbose_eval\n",
    ")\n",
    "\n",
    "auc_train = np.round(h._best_score['train']['auc'], 4)\n",
    "auc_val = np.round(h._best_score['valid']['auc'], 4)\n",
    "print('Best auc score:\\ntrain:{}\\nvalid:{}'.format(auc_train, auc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\ttrain's auc: 0.651491\tvalid's auc: 0.667144\n",
      "[100]\ttrain's auc: 0.65151\tvalid's auc: 0.667192\n",
      "[150]\ttrain's auc: 0.651558\tvalid's auc: 0.667192\n",
      "[200]\ttrain's auc: 0.651607\tvalid's auc: 0.667192\n",
      "[250]\ttrain's auc: 0.651624\tvalid's auc: 0.667179\n",
      "[300]\ttrain's auc: 0.651708\tvalid's auc: 0.667183\n",
      "[350]\ttrain's auc: 0.651856\tvalid's auc: 0.667183\n",
      "[400]\ttrain's auc: 0.652105\tvalid's auc: 0.667183\n",
      "[450]\ttrain's auc: 0.652115\tvalid's auc: 0.667187\n",
      "[500]\ttrain's auc: 0.652176\tvalid's auc: 0.667187\n",
      "Best auc score:\n",
      "train:0.6522\n",
      "valid:0.6672\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND VALIDATE\n",
    "col = 'C7'\n",
    "num_verbose_eval = 50\n",
    "lgbm = lgb.LGBMClassifier(n_estimators=500, objective='binary', num_leaves=8, learning_rate=0.02, metric='auc')\n",
    "h = lgbm.fit(\n",
    "    train[[col]], \n",
    "    train.isFraud,     \n",
    "    eval_metric='auc', \n",
    "    eval_set=[(train[[col]],train.isFraud),\n",
    "              (validate[[col]],validate.isFraud)],\n",
    "    eval_names=['train', 'valid'],\n",
    "    verbose=num_verbose_eval\n",
    ")\n",
    "\n",
    "auc_train = np.round(h._best_score['train']['auc'], 4)\n",
    "auc_val = np.round(h._best_score['valid']['auc'], 4)\n",
    "print('Best auc score:\\ntrain:{}\\nvalid:{}'.format(auc_train, auc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\ttrain's auc: 0.656968\tvalid's auc: 0.581784\n",
      "[100]\ttrain's auc: 0.65703\tvalid's auc: 0.58176\n",
      "[150]\ttrain's auc: 0.657097\tvalid's auc: 0.581759\n",
      "[200]\ttrain's auc: 0.657147\tvalid's auc: 0.581752\n",
      "[250]\ttrain's auc: 0.657204\tvalid's auc: 0.581873\n",
      "[300]\ttrain's auc: 0.657252\tvalid's auc: 0.581875\n",
      "[350]\ttrain's auc: 0.657478\tvalid's auc: 0.581875\n",
      "[400]\ttrain's auc: 0.657479\tvalid's auc: 0.581876\n",
      "[450]\ttrain's auc: 0.657503\tvalid's auc: 0.581875\n",
      "[500]\ttrain's auc: 0.657508\tvalid's auc: 0.581875\n",
      "Best auc score:\n",
      "train:0.6575\n",
      "valid:0.5819\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND VALIDATE\n",
    "col = 'C8'\n",
    "num_verbose_eval = 50\n",
    "lgbm = lgb.LGBMClassifier(n_estimators=500, objective='binary', num_leaves=8, learning_rate=0.02, metric='auc')\n",
    "h = lgbm.fit(\n",
    "    train[[col]], \n",
    "    train.isFraud,     \n",
    "    eval_metric='auc', \n",
    "    eval_set=[(train[[col]],train.isFraud),\n",
    "              (validate[[col]],validate.isFraud)],\n",
    "    eval_names=['train', 'valid'],\n",
    "    verbose=num_verbose_eval\n",
    ")\n",
    "\n",
    "auc_train = np.round(h._best_score['train']['auc'], 4)\n",
    "auc_val = np.round(h._best_score['valid']['auc'], 4)\n",
    "print('Best auc score:\\ntrain:{}\\nvalid:{}'.format(auc_train, auc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to remove weak variables from your model and then evaluate your entire model with your normal local validation to see if AUC increases or decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas_range = pd.data_range(start='2017-10-01', end='2019-01-01')\n",
    "us_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\n",
    "\n",
    "for df in [train_transaction_data, test_transaction_data]:\n",
    "    df['DT'] = df['TransactionDT'].apply(lambda x:(START_DATE + timedelta(seconds=x)))\n",
    "    df['DT_M'] = ((df['DT'].dt.year - 2017)*12 + df['DT'].dt.month).astype(np.int8)\n",
    "    df['DT_W'] = ((df['DT'].dt.year - 2017)*52 + df['DT'].dt.weekofyear).astype(np.int8)\n",
    "    df['DT_D'] = ((df['DT'].dt.year - 2017)*365 + df['DT'].dt.dayofyear).astype(np.int8)\n",
    "    \n",
    "    df['DT_hour'] = (df['DT'].dt.hour).astype(np.int8)\n",
    "    df['DT_day_week'] = (df['DT'].dt.dayofweek).astype(np.int8)\n",
    "    df['DT_day_month'] = (df['DT'].dt.day).astype(np.int8)\n",
    "    df['DT_week_month'] = (df['DT'].dt.day) / 7\n",
    "    df['DT_week_month'] = df['DT_week_month'].apply(lambda x: math.ceil(x))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_card1_df = train_transaction_data['card1'].value_counts().reset_index()\n",
    "test_card1_df = test_transaction_data['card1'].value_counts().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14932"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_transaction_data.groupby('card1').size() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daset[c+'_freq'] = daset[c].map(daset.groupby(c).size() / daset.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
