{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import warnings\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import itertools\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# visualization \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data preprocessing \n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# model\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# eveluation \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_curve\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# hyperparameters tuning \n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "\n",
    "# utils\n",
    "import sys\n",
    "sys.path.insert(0, '/tf/notebooks/other/kaggle')\n",
    "from utils import utils_featexp\n",
    "from utils import utils_features_engineering\n",
    "from utils import utils_features_plots\n",
    "from utils import utils_reduce_memory\n",
    "from utils import utils_statistic\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 9527"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = Path('../input/ieee-cis-fraud-detection/')\n",
    "\n",
    "with open(str(main_path / 'train_df.pkl'), 'rb') as handle:\n",
    "    train_df = pickle.load(handle)\n",
    "    \n",
    "with open(str(main_path / 'test_df.pkl'), 'rb') as handle:\n",
    "    test_df = pickle.load(handle)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_split(df):\n",
    "    df['device_name'] = df['DeviceInfo'].str.split('/', expand=True)[0]\n",
    "    df['device_version'] = df['DeviceInfo'].str.split('/', expand=True)[1]\n",
    "    df['OS_id_30'] = df['id_30'].str.split(' ', expand=True)[0]\n",
    "    df['browser_id_31'] = df['id_31'].str.split(' ', expand=True)[0]\n",
    "\n",
    "    df.loc[df['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n",
    "    df.loc[df['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n",
    "    df.loc[df['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n",
    "    df.loc[df['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n",
    "    df.loc[df['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n",
    "    df.loc[df['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n",
    "    df.loc[df['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n",
    "    df.loc[df['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n",
    "    df.loc[df['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n",
    "    df.loc[df['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n",
    "    df.loc[df['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n",
    "    df.loc[df['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n",
    "    df.loc[df['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n",
    "    df.loc[df['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n",
    "    df.loc[df['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n",
    "    df.loc[df['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n",
    "    df.loc[df['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n",
    "    \n",
    "    #Classes that are too sparse are placed in other classes\n",
    "    df.loc[df.device_name.isin(df.device_name.value_counts()[df.device_name.value_counts() < 100].index), 'device_name'] = \"Others\"\n",
    "    df['had_id'] = 1\n",
    "    gc.collect()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = id_split(train_df)\n",
    "test_df = id_split(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows            47722\n",
      "iOS Device         19782\n",
      "MacOS              12573\n",
      "Samsung            12092\n",
      "Trident             7440\n",
      "Others              4978\n",
      "RV                  4385\n",
      "Motorola            2935\n",
      "Huawei              2377\n",
      "LG                  2331\n",
      "Sony                 575\n",
      "ZTE                  518\n",
      "HTC                  406\n",
      "hi6210sft Build      190\n",
      "F3213 Build          125\n",
      "Linux                121\n",
      "F5121 Build          116\n",
      "Name: device_name, dtype: int64\n",
      "7.0                      7440\n",
      "NRD90M                   5908\n",
      "MMB29K                   1874\n",
      "MRA58K                   1446\n",
      "MMB29M                   1342\n",
      "LRX22G                    757\n",
      "NMF26X                    754\n",
      "LMY48B                    740\n",
      "LMY47V                    637\n",
      "NRD90U                    564\n",
      "R16NW                     494\n",
      "MMB29T                    418\n",
      "LMY47I                    413\n",
      "HuaweiALE-L23             312\n",
      "NMA26.42-69               285\n",
      "KTU84P                    276\n",
      "NPJ25.93-14.7             219\n",
      "KOT49H                    202\n",
      "MXB48T                    179\n",
      "HUAWEICAM-L03             172\n",
      "LVY48F                    167\n",
      "NPJS25.93-14-13           147\n",
      "LMY47X                    145\n",
      "HUAWEIPRA-LX3             135\n",
      "NPN25.137-92              130\n",
      "MPIS24.241-15.3-7         129\n",
      "NPSS26.118-19-14          129\n",
      "LRX21T                    128\n",
      "HUAWEITRT-L53             122\n",
      "HUAWEITAG-L13             113\n",
      "                         ... \n",
      "NRD90M.046                  3\n",
      "HUAWEIRNE-L22               2\n",
      "NPHS25.200-23-1             2\n",
      "H90022b                     2\n",
      "KXB20.9-1.10-1.24-1.1       2\n",
      "HUAWEIWAS-L03T              2\n",
      "E050L                       2\n",
      "NRD90M.050                  2\n",
      "H44312g                     2\n",
      "MPIS24.241-2.35-1-17        2\n",
      "4.28.502.2                  1\n",
      "V49520l                     1\n",
      "HUAWEIANE-LX3               1\n",
      "H81021z                     1\n",
      "OPR4.170623.006             1\n",
      "34.2.A.2.47                 1\n",
      "V100                        1\n",
      "HONORPLK-L01                1\n",
      "V41020c                     1\n",
      "NPN25.137-15                1\n",
      "HUAWEILUA-U23               1\n",
      "NPKS25.200-12-9             1\n",
      "NJH47F                      1\n",
      "OPN27.76-12-22              1\n",
      "HONORBND-L21                1\n",
      "H81022f                     1\n",
      "HUAWEILDN-LX3               1\n",
      "HUAWEILYO-L21               1\n",
      "Q1010                       1\n",
      "2.12.111.1                  1\n",
      "Name: device_version, Length: 293, dtype: int64\n",
      "Windows    36739\n",
      "iOS        19782\n",
      "Mac        13580\n",
      "Android     6303\n",
      "Linux       1136\n",
      "other         15\n",
      "func          10\n",
      "Name: OS_id_30, dtype: int64\n",
      "chrome               76059\n",
      "mobile               28379\n",
      "ie                    9733\n",
      "safari                8913\n",
      "firefox               7012\n",
      "edge                  6401\n",
      "samsung               2044\n",
      "opera                  449\n",
      "android                386\n",
      "other                  312\n",
      "Samsung/SM-G532M       150\n",
      "google                 146\n",
      "Generic/Android        138\n",
      "Samsung/SM-G531H        52\n",
      "Microsoft/Windows       25\n",
      "silk                    19\n",
      "ZTE/Blade                9\n",
      "line                     6\n",
      "maxthon                  6\n",
      "comodo                   6\n",
      "icedragon                5\n",
      "Mozilla/Firefox          5\n",
      "aol                      5\n",
      "Lanix/Ilium              3\n",
      "facebook                 2\n",
      "puffin                   2\n",
      "waterfox                 2\n",
      "palemoon                 2\n",
      "chromium                 1\n",
      "cyberfox                 1\n",
      "M4Tel/M4                 1\n",
      "Nokia/Lumia              1\n",
      "Samsung/SCH              1\n",
      "LG/K-200                 1\n",
      "iron                     1\n",
      "Inco/Minion              1\n",
      "seamonkey                1\n",
      "Cherry                   1\n",
      "BLU/Dash                 1\n",
      "Name: browser_id_31, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Five new fields have been added\n",
    "print(train_df.device_name.value_counts())\n",
    "print(train_df.device_version.value_counts())\n",
    "print(train_df.OS_id_30.value_counts())\n",
    "print(train_df.browser_id_31.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df['DT_D'] = ((df['TransactionDT'].dt.year - 2017)*365 + df['TransactionDT'].dt.dayofyear).astype(np.int16)\n",
    "    df['DT_W'] = (df['TransactionDT'].dt.year - 2017)*52 + df['TransactionDT'].dt.weekofyear\n",
    "    df['DT_M'] = (df['TransactionDT'].dt.year - 2017)*12 + df['TransactionDT'].dt.month\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为意识到ProductCD各个种类在欺诈和时间序列上的表现差异很大，所以将他们的count_encoding拆解为5个指标\n",
    "\n",
    "Recognizing that the various categories of ProductCD behave differently in terms of fraud and time series, we split their count_encoding feature into five indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Count isFraud with group by ProductCD-DT_D combination...\n",
      "[INFO] Count TransactionAmt with group by ProductCD-DT_D combination...\n",
      "[INFO] Count isFraud with group by ProductCD-DT_D combination...\n",
      "[INFO] Count TransactionAmt with group by ProductCD-DT_D combination...\n",
      "[INFO] Count isFraud with group by ProductCD-DT_D combination...\n",
      "[INFO] Count TransactionAmt with group by ProductCD-DT_D combination...\n",
      "[INFO] Count isFraud with group by ProductCD-DT_D combination...\n",
      "[INFO] Count TransactionAmt with group by ProductCD-DT_D combination...\n",
      "[INFO] Count isFraud with group by ProductCD-DT_D combination...\n",
      "[INFO] Count TransactionAmt with group by ProductCD-DT_D combination...\n"
     ]
    }
   ],
   "source": [
    "for i in train_df['ProductCD'].unique():\n",
    "    new_col_name = 'c_product_{}_day'.format(i)\n",
    "    train_df = utils_features_engineering.do_count(data=train_df, \n",
    "                                                   group_cols=['ProductCD', 'DT_D'],\n",
    "                                                   target_col='isFraud', \n",
    "                                                   new_col_name=new_col_name, \n",
    "                                                   col_type=np.int16)\n",
    "    \n",
    "    test_df = utils_features_engineering.do_count(data=test_df, \n",
    "                                              group_cols=['ProductCD', 'DT_D'],\n",
    "                                              target_col='TransactionAmt', \n",
    "                                              new_col_name=new_col_name, \n",
    "                                              col_type=np.int16)\n",
    "    train_df.loc[train_df.ProductCD != i, new_col_name] = -999\n",
    "    test_df.loc[test_df.ProductCD != i, new_col_name] = -999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use card features to create unique id**\n",
    "\n",
    "open card(開卡)：第一筆消費的產生，為日期 - D1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['open_card'] = train_df.DT_D - train_df.D1\n",
    "train_df['first_tran'] = train_df.DT_D - train_df.D2\n",
    "\n",
    "test_df['open_card'] = test_df.DT_D - test_df.D1\n",
    "test_df['first_tran'] = test_df.DT_D - test_df.D2\n",
    "\n",
    "train_df['uid1'] = train_df.card1.astype(str) + '_' + train_df.card2.astype(str) + '_' + train_df.card3.astype(str) \\\n",
    "+ '_' + train_df.card4.astype(str) + '_' + train_df.card5.astype(str) + '_' + train_df.card6.astype(str) \\\n",
    "+ '_' + train_df.addr1.astype(str) + '_' + train_df.addr2.astype(str) + '_' + train_df.open_card.astype(str)\n",
    "\n",
    "test_df['uid1'] = test_df.card1.astype(str) + '_' + test_df.card2.astype(str) + '_' + test_df.card3.astype(str) \\\n",
    "+ '_' + test_df.card4.astype(str) + '_' + test_df.card5.astype(str) + '_' + test_df.card6.astype(str) \\\n",
    "+ '_' + test_df.addr1.astype(str) + '_' + test_df.addr2.astype(str) + '_' + test_df.open_card.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique uid of train data:222518\n",
      "Unique uid of train data:198011\n"
     ]
    }
   ],
   "source": [
    "print('Unique uid of train data:{}'.format(train_df['uid1'].nunique()))\n",
    "print('Unique uid of train data:{}'.format(test_df['uid1'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify a device using id_30, id_31, id_32, id_33, DeviceType, DeviceInfo\n",
    "import hashlib\n",
    "def device_hash(x):\n",
    "    s =  str(x['id_30']) + str(x['id_31']) + str(x['id_32']) + str(x['id_33']) + str(x['DeviceType']) \\\n",
    "        + str(x['DeviceInfo'])\n",
    "    h = hashlib.sha256(s.encode('utf-8')).hexdigest()[0:15]\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df['device_hash'] = df.apply(lambda x: device_hash(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of devices with the same user\n",
    "concat_df = pd.concat([train_df[['uid1', 'device_hash']], test_df[['uid1', 'device_hash']]])\n",
    "tmp = concat_df.groupby('uid1')['device_hash'].agg(['nunique'])\n",
    "train_df['uid_device_nunique'] = train_df.uid1.map(tmp.to_dict()['nunique'])\n",
    "test_df['uid_device_nunique'] = train_df.uid1.map(tmp.to_dict()['nunique'])\n",
    "del concat_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到金額不為零小數位的個數\n",
    "# get number of non-zero decimal digits\n",
    "def change(hoge):\n",
    "    hoge = np.round(hoge, 3)\n",
    "    num = 3\n",
    "    hoge = int(np.round(np.round(hoge,3)*1000))\n",
    "    while(hoge % 10 == 0):\n",
    "        num = num-1\n",
    "        hoge = hoge /10\n",
    "    if num < 0:\n",
    "        num = 0\n",
    "    return num\n",
    "  \n",
    "train_df['decimal_digit'] = train_df[\"TransactionAmt\"].map(change)\n",
    "test_df['decimal_digit'] = test_df['TransactionAmt'].map(change)\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 没有 identity 填充為0\n",
    "train_df.had_id = train_df.had_id.fillna(0)\n",
    "test_df.had_id = test_df.had_id.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### D系列数据有随時間增加的趋势，未来的数据大于过去的数据，所以进行缩放，将相对关系保留\n",
    "# D series data has a trend of increasing with time. Future data is larger than past data, \n",
    "# so zoom in and keep the relative relationship.\n",
    "for t in ['D1', 'D2', 'D4', 'D6', 'D10', 'D11', 'D12', 'D14', 'D15']:\n",
    "    train_df[t + '_revised'] = train_df[t] / train_df.groupby('DT_W')[t].transform('max')\n",
    "    test_df[t + '_revised'] = test_df[t] / test_df.groupby('DT_W')[t].transform('max')\n",
    "for t in ['D3','D5','D7','D8','D13']:\n",
    "    train_df[t + '_revised'] = train_df[t] / train_df.groupby('DT_M')[t].transform('max')\n",
    "    test_df[t + '_revised'] = test_df[t] / test_df.groupby('DT_M')[t].transform('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 对时间进行细分，周内第几天和当天时间小时\n",
    "# Subdivide the time into days of the week and hours of the day.\n",
    "train_df['dow'] = train_df['TransactionDT'].dt.dayofweek\n",
    "train_df['hour'] = train_df['TransactionDT'].dt.hour\n",
    "test_df['dow'] = test_df['TransactionDT'].dt.dayofweek\n",
    "test_df['hour'] = test_df['TransactionDT'].dt.hour\n",
    "train_df['email_domain_comp'] = (train_df['P_emaildomain'].values == train_df['R_emaildomain'].values).astype(int)\n",
    "test_df['email_domain_comp'] = (test_df['P_emaildomain'].values == test_df['R_emaildomain'].values).astype(int)\n",
    "train_df.drop(['D9'],axis=1,inplace=True)\n",
    "test_df.drop(['D9'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类别变量，需要进行LabelEncoder\n",
    "cat_columns = ['uid1','id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', \n",
    "               'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32',\n",
    "               'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'ProductCD', \n",
    "               'M4','P_emaildomain', 'R_emaildomain', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', \n",
    "               'addr1', 'addr2', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9', 'hour', 'dow','device_name', \n",
    "               'device_version', 'OS_id_30', 'browser_id_31']\n",
    "\n",
    "# 进行count encoding的\n",
    "count_columns = ['uid1', 'id_13', 'id_14', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_24', 'id_25',\n",
    "                 'id_26', 'id_30', 'id_31', 'id_33', 'DeviceInfo', 'P_emaildomain', 'R_emaildomain', \n",
    "                 'card1', 'card2','card3','card5', 'card6', 'addr1','addr2','hour','device_version',\n",
    "                 'OS_id_30', 'browser_id_31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for f in cat_columns:\n",
    "    #if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train_df[f].astype(str)) + list(test_df[f].astype(str)))\n",
    "    train_df[f] = lbl.transform(list(train_df[f].astype(str)))\n",
    "    test_df[f] = lbl.transform(list(test_df[f].astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.fillna(-999,inplace = True)\n",
    "test_df.fillna(-999,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in count_columns:\n",
    "    train_df[i+'_count_full'] = train_df[i].map(pd.concat([train_df[i], \n",
    "                                                           test_df[i]], ignore_index=True).value_counts(dropna=False))\n",
    "    test_df[i+'_count_full'] = test_df[i].map(pd.concat([train_df[i], \n",
    "                                                         test_df[i]], ignore_index=True).value_counts(dropna=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of transactions in natural hours and days\n",
    "train_len = len(train_df)\n",
    "train_test_all = pd.concat([train_df[['TransactionDT', 'TransactionAmt']],\n",
    "                            test_df[['TransactionDT', 'TransactionAmt']]],ignore_index=True,sort=False)\n",
    "train_test_all['day_count'] = train_test_all.groupby(train_test_all.TransactionDT.dt.date)['TransactionAmt'].transform('count')\n",
    "train_test_all['hour_count'] = train_test_all.groupby(train_test_all.TransactionDT.map(lambda x:str(x)[:13]))['TransactionAmt'].transform('count')\n",
    "train_df['day_count'] = train_test_all[:train_len].day_count.tolist()\n",
    "test_df['day_count'] = train_test_all[train_len:].day_count.tolist()\n",
    "train_df['hour_count'] = train_test_all[:train_len].hour_count.tolist()\n",
    "test_df['hour_count'] = train_test_all[train_len:].hour_count.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 按照价格个类别确定商品id\n",
    "# Identify commodity ID by price category\n",
    "temp123 = ['TransactionAmt__ProductCD']\n",
    "for feature in temp123:\n",
    "    f1, f2 = feature.split('__')\n",
    "    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n",
    "    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n",
    "    train[feature] = le.transform(list(train[feature].astype(str).values))\n",
    "    test[feature] = le.transform(list(test[feature].astype(str).values))\n",
    "train.rename(columns = {'TransactionAmt__ProductCD':'ProductID'},inplace=True)\n",
    "test.rename(columns = {'TransactionAmt__ProductCD':'ProductID'},inplace=True)\n",
    "for i in ['ProductID']:\n",
    "    train[i+'_count_full'] = train[i].map(pd.concat([train[i], test[i]], ignore_index=True).value_counts(dropna=False))\n",
    "    test[i+'_count_full'] = test[i].map(pd.concat([train[i], test[i]], ignore_index=True).value_counts(dropna=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
