{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train.csv.zip',\n",
       " 'test.csv',\n",
       " 'sample_submission.csv.zip',\n",
       " 'test.csv.zip',\n",
       " 'train.csv']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "IS_LOCAL = True\n",
    "if(IS_LOCAL):\n",
    "    PATH = Path('/mnt/disks/data/santander-customer-transaction/')\n",
    "else:\n",
    "    PATH = Path('../input/')\n",
    "    \n",
    "os.listdir(str(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.1 s, sys: 769 ms, total: 15.9 s\n",
      "Wall time: 15.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df = pd.read_csv(str(PATH / 'train.csv'))\n",
    "test_df = pd.read_csv(str(PATH / 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_df.drop(['target', 'ID_code'], axis=1)\n",
    "test_features = test_df.drop(['ID_code'], axis=1)\n",
    "train_target = train_df['target']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "train_features = sc.fit_transform(train_features)\n",
    "test_features = sc.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_split = 11\n",
    "splits = list(StratifiedKFold(n_splits=n_split, shuffle=True).split(\n",
    "    train_features, train_target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cycling learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(object):\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3, step_size=2000, \n",
    "                 mode='triangular', gamma=1., scale_fn=None, scale_mode='cycle',\n",
    "                 last_batch_iteration=-1):\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)    \n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "        \n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "        \n",
    "        self.step_size = step_size\n",
    "        \n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'trangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        \n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "        \n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "    \n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2.**(x-1))\n",
    "    \n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1-x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Simple NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.75):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.relu = nn.ReLU()\n",
    "        self.droput = dropout\n",
    "        self.fc1 = nn.Linear(1, hidden_dim)\n",
    "        self.fc2 = nn.Linear(int(hidden_dim * input_dim), 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bias_size = x.size(0)\n",
    "        x = x.view(-1, 1)\n",
    "        y = self.fc1(x)\n",
    "        y = self.relu(y)\n",
    "        y = y.view(bias_size, -1)        \n",
    "        output = self.fc2(y)\n",
    "        return output    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 256\n",
    "\n",
    "pred_train = np.zeros(train_features.shape[0])\n",
    "pred_test = np.zeros(test_features.shape[0])\n",
    "\n",
    "x_test = np.array(test_features)\n",
    "x_test_cuda = torch.tensor(x_test, dtype=torch.float).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "total_train_loss = []\n",
    "total_val_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1 =====\n",
      "Epoch 1/40, loss:0.2328, val_loss:0.2173, time:3.93s\n",
      "Epoch 2/40, loss:0.2160, val_loss:0.2436, time:3.35s\n",
      "Epoch 3/40, loss:0.2127, val_loss:0.2219, time:3.33s\n",
      "Epoch 4/40, loss:0.2089, val_loss:0.2083, time:3.32s\n",
      "Epoch 5/40, loss:0.2052, val_loss:0.2125, time:3.38s\n",
      "Epoch 6/40, loss:0.2027, val_loss:0.2096, time:3.33s\n",
      "Epoch 7/40, loss:0.2046, val_loss:0.2083, time:3.34s\n",
      "Epoch 8/40, loss:0.2051, val_loss:0.2093, time:3.38s\n",
      "Epoch 9/40, loss:0.2065, val_loss:0.2104, time:3.32s\n",
      "Epoch 10/40, loss:0.2045, val_loss:0.2120, time:3.34s\n",
      "Epoch 11/40, loss:0.2018, val_loss:0.2098, time:3.36s\n",
      "Epoch 12/40, loss:0.2010, val_loss:0.2099, time:3.42s\n",
      "Epoch 13/40, loss:0.2022, val_loss:0.2097, time:3.34s\n",
      "Epoch 14/40, loss:0.2041, val_loss:0.2077, time:3.33s\n",
      "Epoch 15/40, loss:0.2032, val_loss:0.2081, time:3.36s\n",
      "Epoch 16/40, loss:0.2017, val_loss:0.2087, time:3.37s\n",
      "Epoch 17/40, loss:0.2007, val_loss:0.2099, time:3.32s\n",
      "Epoch 18/40, loss:0.2002, val_loss:0.2118, time:3.42s\n",
      "Epoch 19/40, loss:0.2019, val_loss:0.2066, time:3.35s\n",
      "Epoch 20/40, loss:0.2029, val_loss:0.2078, time:3.34s\n",
      "Epoch 21/40, loss:0.2019, val_loss:0.2157, time:3.32s\n",
      "Epoch 22/40, loss:0.2005, val_loss:0.2068, time:3.40s\n",
      "Epoch 23/40, loss:0.1994, val_loss:0.2067, time:3.33s\n",
      "Epoch 24/40, loss:0.2000, val_loss:0.2069, time:3.35s\n",
      "Epoch 25/40, loss:0.2008, val_loss:0.2075, time:3.41s\n",
      "Epoch 26/40, loss:0.2016, val_loss:0.2066, time:3.34s\n",
      "Epoch 27/40, loss:0.2001, val_loss:0.2089, time:3.32s\n",
      "Epoch 28/40, loss:0.1994, val_loss:0.2067, time:3.43s\n",
      "Epoch 29/40, loss:0.1991, val_loss:0.2080, time:3.35s\n",
      "Epoch 30/40, loss:0.1998, val_loss:0.2069, time:3.33s\n",
      "Epoch 31/40, loss:0.2004, val_loss:0.2080, time:3.32s\n",
      "Epoch 32/40, loss:0.1997, val_loss:0.2076, time:3.39s\n",
      "Epoch 33/40, loss:0.1996, val_loss:0.2098, time:3.34s\n",
      "Epoch 34/40, loss:0.1988, val_loss:0.2065, time:3.31s\n",
      "Epoch 35/40, loss:0.1987, val_loss:0.2067, time:3.38s\n",
      "Epoch 36/40, loss:0.1994, val_loss:0.2092, time:3.32s\n",
      "Epoch 37/40, loss:0.1997, val_loss:0.2079, time:3.33s\n",
      "Epoch 38/40, loss:0.1992, val_loss:0.2066, time:3.39s\n",
      "Epoch 39/40, loss:0.1990, val_loss:0.2070, time:3.32s\n",
      "Epoch 40/40, loss:0.1981, val_loss:0.2084, time:3.33s\n",
      "===== Fold 2 =====\n",
      "Epoch 1/40, loss:0.2393, val_loss:0.2151, time:3.35s\n",
      "Epoch 2/40, loss:0.2162, val_loss:0.2116, time:3.28s\n",
      "Epoch 3/40, loss:0.2153, val_loss:0.2170, time:3.30s\n",
      "Epoch 4/40, loss:0.2095, val_loss:0.2060, time:3.36s\n",
      "Epoch 5/40, loss:0.2068, val_loss:0.2062, time:3.26s\n",
      "Epoch 6/40, loss:0.2038, val_loss:0.2054, time:3.26s\n",
      "Epoch 7/40, loss:0.2054, val_loss:0.2137, time:3.32s\n",
      "Epoch 8/40, loss:0.2064, val_loss:0.2057, time:3.30s\n",
      "Epoch 9/40, loss:0.2067, val_loss:0.2043, time:3.28s\n",
      "Epoch 10/40, loss:0.2036, val_loss:0.2059, time:3.31s\n",
      "Epoch 11/40, loss:0.2025, val_loss:0.2058, time:3.36s\n",
      "Epoch 12/40, loss:0.2015, val_loss:0.2212, time:3.30s\n",
      "Epoch 13/40, loss:0.2031, val_loss:0.2069, time:3.27s\n",
      "Epoch 14/40, loss:0.2034, val_loss:0.2037, time:3.36s\n",
      "Epoch 15/40, loss:0.2040, val_loss:0.2062, time:3.29s\n",
      "Epoch 16/40, loss:0.2018, val_loss:0.2057, time:3.27s\n",
      "Epoch 17/40, loss:0.2007, val_loss:0.2043, time:3.33s\n",
      "Epoch 18/40, loss:0.2007, val_loss:0.2030, time:3.29s\n",
      "Epoch 19/40, loss:0.2021, val_loss:0.2081, time:3.30s\n",
      "Epoch 20/40, loss:0.2022, val_loss:0.2091, time:3.29s\n",
      "Epoch 21/40, loss:0.2020, val_loss:0.2052, time:3.34s\n",
      "Epoch 22/40, loss:0.2013, val_loss:0.2036, time:3.31s\n",
      "Epoch 23/40, loss:0.2002, val_loss:0.2033, time:3.29s\n",
      "Epoch 24/40, loss:0.2005, val_loss:0.2034, time:3.36s\n",
      "Epoch 25/40, loss:0.2010, val_loss:0.2082, time:3.28s\n",
      "Epoch 26/40, loss:0.2013, val_loss:0.2038, time:3.30s\n",
      "Epoch 27/40, loss:0.2012, val_loss:0.2073, time:3.33s\n",
      "Epoch 28/40, loss:0.2003, val_loss:0.2060, time:3.30s\n",
      "Epoch 29/40, loss:0.1996, val_loss:0.2048, time:3.29s\n",
      "Epoch 30/40, loss:0.2003, val_loss:0.2037, time:3.29s\n",
      "Epoch 31/40, loss:0.2005, val_loss:0.2034, time:3.32s\n",
      "Epoch 32/40, loss:0.2008, val_loss:0.2038, time:3.30s\n",
      "Epoch 33/40, loss:0.1998, val_loss:0.2036, time:3.30s\n",
      "Epoch 34/40, loss:0.1994, val_loss:0.2035, time:3.36s\n",
      "Epoch 35/40, loss:0.1996, val_loss:0.2095, time:3.26s\n",
      "Epoch 36/40, loss:0.2001, val_loss:0.2042, time:3.28s\n",
      "Epoch 37/40, loss:0.2002, val_loss:0.2052, time:3.31s\n",
      "Epoch 38/40, loss:0.2000, val_loss:0.2064, time:3.28s\n",
      "Epoch 39/40, loss:0.2000, val_loss:0.2050, time:3.30s\n",
      "Epoch 40/40, loss:0.1992, val_loss:0.2110, time:3.30s\n",
      "===== Fold 3 =====\n",
      "Epoch 1/40, loss:0.2316, val_loss:0.2147, time:3.27s\n",
      "Epoch 2/40, loss:0.2125, val_loss:0.2162, time:3.31s\n",
      "Epoch 3/40, loss:0.2104, val_loss:0.2085, time:3.33s\n",
      "Epoch 4/40, loss:0.2060, val_loss:0.2064, time:3.28s\n",
      "Epoch 5/40, loss:0.2032, val_loss:0.2065, time:3.29s\n",
      "Epoch 6/40, loss:0.2009, val_loss:0.2047, time:3.35s\n",
      "Epoch 7/40, loss:0.2020, val_loss:0.2076, time:3.29s\n",
      "Epoch 8/40, loss:0.2045, val_loss:0.2157, time:3.32s\n",
      "Epoch 9/40, loss:0.2064, val_loss:0.2094, time:3.29s\n",
      "Epoch 10/40, loss:0.2029, val_loss:0.2051, time:3.36s\n",
      "Epoch 11/40, loss:0.2004, val_loss:0.2061, time:3.26s\n",
      "Epoch 12/40, loss:0.1993, val_loss:0.2048, time:3.25s\n",
      "Epoch 13/40, loss:0.2011, val_loss:0.2126, time:3.26s\n",
      "Epoch 14/40, loss:0.2022, val_loss:0.2062, time:3.27s\n",
      "Epoch 15/40, loss:0.2014, val_loss:0.2059, time:3.30s\n",
      "Epoch 16/40, loss:0.2000, val_loss:0.2116, time:3.32s\n",
      "Epoch 17/40, loss:0.1984, val_loss:0.2087, time:3.27s\n",
      "Epoch 18/40, loss:0.1983, val_loss:0.2054, time:3.30s\n",
      "Epoch 19/40, loss:0.1998, val_loss:0.2110, time:3.27s\n",
      "Epoch 20/40, loss:0.2010, val_loss:0.2075, time:3.36s\n",
      "Epoch 21/40, loss:0.2002, val_loss:0.2052, time:3.27s\n",
      "Epoch 22/40, loss:0.1984, val_loss:0.2069, time:3.30s\n",
      "Epoch 23/40, loss:0.1977, val_loss:0.2053, time:3.36s\n",
      "Epoch 24/40, loss:0.1985, val_loss:0.2047, time:3.27s\n",
      "Epoch 25/40, loss:0.1994, val_loss:0.2049, time:3.30s\n",
      "Epoch 26/40, loss:0.1993, val_loss:0.2056, time:3.34s\n",
      "Epoch 27/40, loss:0.1984, val_loss:0.2078, time:3.28s\n",
      "Epoch 28/40, loss:0.1977, val_loss:0.2053, time:3.27s\n",
      "Epoch 29/40, loss:0.1974, val_loss:0.2045, time:3.27s\n",
      "Epoch 30/40, loss:0.1975, val_loss:0.2049, time:3.39s\n",
      "Epoch 31/40, loss:0.1983, val_loss:0.2085, time:3.29s\n",
      "Epoch 32/40, loss:0.1983, val_loss:0.2041, time:3.30s\n",
      "Epoch 33/40, loss:0.1978, val_loss:0.2055, time:3.35s\n",
      "Epoch 34/40, loss:0.1967, val_loss:0.2055, time:3.31s\n",
      "Epoch 35/40, loss:0.1969, val_loss:0.2045, time:3.28s\n",
      "Epoch 36/40, loss:0.1974, val_loss:0.2111, time:3.33s\n",
      "Epoch 37/40, loss:0.1975, val_loss:0.2059, time:3.29s\n",
      "Epoch 38/40, loss:0.1972, val_loss:0.2050, time:3.32s\n",
      "Epoch 39/40, loss:0.1968, val_loss:0.2045, time:3.32s\n",
      "Epoch 40/40, loss:0.1965, val_loss:0.2092, time:3.35s\n",
      "===== Fold 4 =====\n",
      "Epoch 1/40, loss:0.2317, val_loss:0.2186, time:3.32s\n",
      "Epoch 2/40, loss:0.2154, val_loss:0.2091, time:3.37s\n",
      "Epoch 3/40, loss:0.2122, val_loss:0.2156, time:3.31s\n",
      "Epoch 4/40, loss:0.2081, val_loss:0.2037, time:3.30s\n",
      "Epoch 5/40, loss:0.2054, val_loss:0.2052, time:3.33s\n",
      "Epoch 6/40, loss:0.2026, val_loss:0.2030, time:3.29s\n",
      "Epoch 7/40, loss:0.2045, val_loss:0.2162, time:3.28s\n",
      "Epoch 8/40, loss:0.2066, val_loss:0.2109, time:3.29s\n",
      "Epoch 9/40, loss:0.2063, val_loss:0.2018, time:3.33s\n",
      "Epoch 10/40, loss:0.2036, val_loss:0.2021, time:3.30s\n",
      "Epoch 11/40, loss:0.2018, val_loss:0.2004, time:3.30s\n",
      "Epoch 12/40, loss:0.2011, val_loss:0.2016, time:3.33s\n",
      "Epoch 13/40, loss:0.2025, val_loss:0.2115, time:3.30s\n",
      "Epoch 14/40, loss:0.2031, val_loss:0.2068, time:3.29s\n",
      "Epoch 15/40, loss:0.2030, val_loss:0.2031, time:3.34s\n",
      "Epoch 16/40, loss:0.2019, val_loss:0.2006, time:3.31s\n",
      "Epoch 17/40, loss:0.2005, val_loss:0.2002, time:3.29s\n",
      "Epoch 18/40, loss:0.2004, val_loss:0.2015, time:3.32s\n",
      "Epoch 19/40, loss:0.2019, val_loss:0.2013, time:3.35s\n",
      "Epoch 20/40, loss:0.2025, val_loss:0.2031, time:3.28s\n",
      "Epoch 21/40, loss:0.2014, val_loss:0.2057, time:3.30s\n",
      "Epoch 22/40, loss:0.2003, val_loss:0.2000, time:3.34s\n",
      "Epoch 23/40, loss:0.1992, val_loss:0.2003, time:3.28s\n",
      "Epoch 24/40, loss:0.2000, val_loss:0.2011, time:3.29s\n",
      "Epoch 25/40, loss:0.2004, val_loss:0.2012, time:3.33s\n",
      "Epoch 26/40, loss:0.2008, val_loss:0.2077, time:3.37s\n",
      "Epoch 27/40, loss:0.2004, val_loss:0.2025, time:3.27s\n",
      "Epoch 28/40, loss:0.1994, val_loss:0.2003, time:3.29s\n",
      "Epoch 29/40, loss:0.1990, val_loss:0.2025, time:3.32s\n",
      "Epoch 30/40, loss:0.2001, val_loss:0.2007, time:3.29s\n",
      "Epoch 31/40, loss:0.1999, val_loss:0.2082, time:3.24s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40, loss:0.2001, val_loss:0.2004, time:3.33s\n",
      "Epoch 33/40, loss:0.1994, val_loss:0.2003, time:3.27s\n",
      "Epoch 34/40, loss:0.1988, val_loss:0.2002, time:3.27s\n",
      "Epoch 35/40, loss:0.1990, val_loss:0.2003, time:3.34s\n",
      "Epoch 36/40, loss:0.1995, val_loss:0.2007, time:3.34s\n",
      "Epoch 37/40, loss:0.1994, val_loss:0.2010, time:3.28s\n",
      "Epoch 38/40, loss:0.1991, val_loss:0.2004, time:3.28s\n",
      "Epoch 39/40, loss:0.1986, val_loss:0.2015, time:3.36s\n",
      "Epoch 40/40, loss:0.1982, val_loss:0.2007, time:3.31s\n",
      "===== Fold 5 =====\n",
      "Epoch 1/40, loss:0.2368, val_loss:0.2094, time:3.33s\n",
      "Epoch 2/40, loss:0.2146, val_loss:0.2076, time:3.31s\n",
      "Epoch 3/40, loss:0.2107, val_loss:0.2080, time:3.32s\n",
      "Epoch 4/40, loss:0.2084, val_loss:0.2054, time:3.29s\n",
      "Epoch 5/40, loss:0.2048, val_loss:0.2052, time:3.34s\n",
      "Epoch 6/40, loss:0.2024, val_loss:0.2045, time:3.31s\n",
      "Epoch 7/40, loss:0.2041, val_loss:0.2107, time:3.28s\n",
      "Epoch 8/40, loss:0.2055, val_loss:0.2049, time:3.34s\n",
      "Epoch 9/40, loss:0.2065, val_loss:0.2099, time:3.27s\n",
      "Epoch 10/40, loss:0.2037, val_loss:0.2037, time:3.28s\n",
      "Epoch 11/40, loss:0.2024, val_loss:0.2092, time:3.33s\n",
      "Epoch 12/40, loss:0.2010, val_loss:0.2059, time:3.29s\n",
      "Epoch 13/40, loss:0.2023, val_loss:0.2040, time:3.28s\n",
      "Epoch 14/40, loss:0.2037, val_loss:0.2043, time:3.31s\n",
      "Epoch 15/40, loss:0.2034, val_loss:0.2053, time:3.38s\n",
      "Epoch 16/40, loss:0.2014, val_loss:0.2065, time:3.27s\n",
      "Epoch 17/40, loss:0.2007, val_loss:0.2029, time:3.28s\n",
      "Epoch 18/40, loss:0.2002, val_loss:0.2040, time:3.30s\n",
      "Epoch 19/40, loss:0.2022, val_loss:0.2141, time:3.27s\n",
      "Epoch 20/40, loss:0.2023, val_loss:0.2040, time:3.28s\n",
      "Epoch 21/40, loss:0.2020, val_loss:0.2037, time:3.30s\n",
      "Epoch 22/40, loss:0.2004, val_loss:0.2030, time:3.28s\n",
      "Epoch 23/40, loss:0.1991, val_loss:0.2036, time:3.27s\n",
      "Epoch 24/40, loss:0.1999, val_loss:0.2043, time:3.30s\n",
      "Epoch 25/40, loss:0.2006, val_loss:0.2054, time:3.31s\n",
      "Epoch 26/40, loss:0.2011, val_loss:0.2051, time:3.26s\n",
      "Epoch 27/40, loss:0.2004, val_loss:0.2040, time:3.28s\n",
      "Epoch 28/40, loss:0.1992, val_loss:0.2038, time:3.34s\n",
      "Epoch 29/40, loss:0.1992, val_loss:0.2052, time:3.31s\n",
      "Epoch 30/40, loss:0.1995, val_loss:0.2127, time:3.30s\n",
      "Epoch 31/40, loss:0.2004, val_loss:0.2067, time:3.37s\n",
      "Epoch 32/40, loss:0.2002, val_loss:0.2031, time:3.30s\n",
      "Epoch 33/40, loss:0.1995, val_loss:0.2078, time:3.32s\n",
      "Epoch 34/40, loss:0.1987, val_loss:0.2060, time:3.27s\n",
      "Epoch 35/40, loss:0.1990, val_loss:0.2039, time:3.40s\n",
      "Epoch 36/40, loss:0.1992, val_loss:0.2039, time:3.29s\n",
      "Epoch 37/40, loss:0.2002, val_loss:0.2041, time:3.27s\n",
      "Epoch 38/40, loss:0.1993, val_loss:0.2031, time:3.37s\n",
      "Epoch 39/40, loss:0.1988, val_loss:0.2025, time:3.30s\n",
      "Epoch 40/40, loss:0.1983, val_loss:0.2056, time:3.28s\n",
      "===== Fold 6 =====\n",
      "Epoch 1/40, loss:0.2414, val_loss:0.2236, time:3.31s\n",
      "Epoch 2/40, loss:0.2171, val_loss:0.2276, time:3.32s\n",
      "Epoch 3/40, loss:0.2155, val_loss:0.2128, time:3.33s\n",
      "Epoch 4/40, loss:0.2126, val_loss:0.2101, time:3.36s\n",
      "Epoch 5/40, loss:0.2084, val_loss:0.2126, time:3.32s\n",
      "Epoch 6/40, loss:0.2058, val_loss:0.2079, time:3.29s\n",
      "Epoch 7/40, loss:0.2058, val_loss:0.2235, time:3.36s\n",
      "Epoch 8/40, loss:0.2083, val_loss:0.2102, time:3.29s\n",
      "Epoch 9/40, loss:0.2078, val_loss:0.2074, time:3.30s\n",
      "Epoch 10/40, loss:0.2048, val_loss:0.2113, time:3.36s\n",
      "Epoch 11/40, loss:0.2030, val_loss:0.2049, time:3.30s\n",
      "Epoch 12/40, loss:0.2025, val_loss:0.2079, time:3.31s\n",
      "Epoch 13/40, loss:0.2034, val_loss:0.2056, time:3.32s\n",
      "Epoch 14/40, loss:0.2038, val_loss:0.2057, time:3.36s\n",
      "Epoch 15/40, loss:0.2039, val_loss:0.2068, time:3.30s\n",
      "Epoch 16/40, loss:0.2022, val_loss:0.2059, time:3.30s\n",
      "Epoch 17/40, loss:0.2013, val_loss:0.2047, time:3.35s\n",
      "Epoch 18/40, loss:0.2008, val_loss:0.2060, time:3.31s\n",
      "Epoch 19/40, loss:0.2016, val_loss:0.2067, time:3.29s\n",
      "Epoch 20/40, loss:0.2025, val_loss:0.2052, time:3.36s\n",
      "Epoch 21/40, loss:0.2018, val_loss:0.2045, time:3.29s\n",
      "Epoch 22/40, loss:0.2011, val_loss:0.2052, time:3.32s\n",
      "Epoch 23/40, loss:0.1998, val_loss:0.2053, time:3.26s\n",
      "Epoch 24/40, loss:0.2005, val_loss:0.2050, time:3.37s\n",
      "Epoch 25/40, loss:0.2015, val_loss:0.2057, time:3.29s\n",
      "Epoch 26/40, loss:0.2014, val_loss:0.2046, time:3.29s\n",
      "Epoch 27/40, loss:0.2004, val_loss:0.2063, time:3.32s\n",
      "Epoch 28/40, loss:0.1997, val_loss:0.2046, time:3.30s\n",
      "Epoch 29/40, loss:0.1997, val_loss:0.2046, time:3.31s\n",
      "Epoch 30/40, loss:0.1999, val_loss:0.2075, time:3.35s\n",
      "Epoch 31/40, loss:0.2007, val_loss:0.2052, time:3.30s\n",
      "Epoch 32/40, loss:0.2007, val_loss:0.2049, time:3.28s\n",
      "Epoch 33/40, loss:0.1999, val_loss:0.2071, time:3.27s\n",
      "Epoch 34/40, loss:0.1989, val_loss:0.2046, time:3.34s\n",
      "Epoch 35/40, loss:0.1992, val_loss:0.2053, time:3.29s\n",
      "Epoch 36/40, loss:0.1996, val_loss:0.2049, time:3.29s\n",
      "Epoch 37/40, loss:0.2003, val_loss:0.2052, time:3.35s\n",
      "Epoch 38/40, loss:0.2000, val_loss:0.2050, time:3.30s\n",
      "Epoch 39/40, loss:0.1993, val_loss:0.2049, time:3.27s\n",
      "Epoch 40/40, loss:0.1989, val_loss:0.2050, time:3.36s\n",
      "===== Fold 7 =====\n",
      "Epoch 1/40, loss:0.2364, val_loss:0.2097, time:3.30s\n",
      "Epoch 2/40, loss:0.2132, val_loss:0.2059, time:3.30s\n",
      "Epoch 3/40, loss:0.2125, val_loss:0.2066, time:3.36s\n",
      "Epoch 4/40, loss:0.2069, val_loss:0.2017, time:3.36s\n",
      "Epoch 5/40, loss:0.2046, val_loss:0.2019, time:3.34s\n",
      "Epoch 6/40, loss:0.2024, val_loss:0.2030, time:3.29s\n",
      "Epoch 7/40, loss:0.2042, val_loss:0.2146, time:3.28s\n",
      "Epoch 8/40, loss:0.2050, val_loss:0.2041, time:3.27s\n",
      "Epoch 9/40, loss:0.2049, val_loss:0.2022, time:3.34s\n",
      "Epoch 10/40, loss:0.2033, val_loss:0.2017, time:3.29s\n",
      "Epoch 11/40, loss:0.2016, val_loss:0.2005, time:3.28s\n",
      "Epoch 12/40, loss:0.2008, val_loss:0.2009, time:3.26s\n",
      "Epoch 13/40, loss:0.2021, val_loss:0.2025, time:3.36s\n",
      "Epoch 14/40, loss:0.2034, val_loss:0.2145, time:3.32s\n",
      "Epoch 15/40, loss:0.2031, val_loss:0.2006, time:3.28s\n",
      "Epoch 16/40, loss:0.2019, val_loss:0.1999, time:3.37s\n",
      "Epoch 17/40, loss:0.2002, val_loss:0.2001, time:3.27s\n",
      "Epoch 18/40, loss:0.2001, val_loss:0.2005, time:3.31s\n",
      "Epoch 19/40, loss:0.2014, val_loss:0.2024, time:3.37s\n",
      "Epoch 20/40, loss:0.2022, val_loss:0.2012, time:3.34s\n",
      "Epoch 21/40, loss:0.2010, val_loss:0.2005, time:3.29s\n",
      "Epoch 22/40, loss:0.2004, val_loss:0.1996, time:3.28s\n",
      "Epoch 23/40, loss:0.1993, val_loss:0.2001, time:3.38s\n",
      "Epoch 24/40, loss:0.1997, val_loss:0.2009, time:3.28s\n",
      "Epoch 25/40, loss:0.2008, val_loss:0.2005, time:3.29s\n",
      "Epoch 26/40, loss:0.2010, val_loss:0.2005, time:3.33s\n",
      "Epoch 27/40, loss:0.1998, val_loss:0.2046, time:3.30s\n",
      "Epoch 28/40, loss:0.1991, val_loss:0.2002, time:3.29s\n",
      "Epoch 29/40, loss:0.1987, val_loss:0.1998, time:3.39s\n",
      "Epoch 30/40, loss:0.1995, val_loss:0.2014, time:3.29s\n",
      "Epoch 31/40, loss:0.2000, val_loss:0.2007, time:3.30s\n",
      "Epoch 32/40, loss:0.2000, val_loss:0.2001, time:3.29s\n",
      "Epoch 33/40, loss:0.1994, val_loss:0.1999, time:3.37s\n",
      "Epoch 34/40, loss:0.1986, val_loss:0.1998, time:3.28s\n",
      "Epoch 35/40, loss:0.1984, val_loss:0.2005, time:3.29s\n",
      "Epoch 36/40, loss:0.1995, val_loss:0.2001, time:3.36s\n",
      "Epoch 37/40, loss:0.1995, val_loss:0.2002, time:3.26s\n",
      "Epoch 38/40, loss:0.1989, val_loss:0.2000, time:3.22s\n",
      "Epoch 39/40, loss:0.1988, val_loss:0.2000, time:3.27s\n",
      "Epoch 40/40, loss:0.1981, val_loss:0.2055, time:3.25s\n",
      "===== Fold 8 =====\n",
      "Epoch 1/40, loss:0.2408, val_loss:0.2121, time:3.30s\n",
      "Epoch 2/40, loss:0.2153, val_loss:0.2092, time:3.33s\n",
      "Epoch 3/40, loss:0.2155, val_loss:0.2058, time:3.28s\n",
      "Epoch 4/40, loss:0.2083, val_loss:0.2028, time:3.30s\n",
      "Epoch 5/40, loss:0.2062, val_loss:0.2031, time:3.36s\n",
      "Epoch 6/40, loss:0.2032, val_loss:0.2022, time:3.33s\n",
      "Epoch 7/40, loss:0.2043, val_loss:0.2010, time:3.26s\n",
      "Epoch 8/40, loss:0.2061, val_loss:0.2012, time:3.37s\n",
      "Epoch 9/40, loss:0.2062, val_loss:0.2018, time:3.32s\n",
      "Epoch 10/40, loss:0.2038, val_loss:0.2003, time:3.28s\n",
      "Epoch 11/40, loss:0.2020, val_loss:0.1997, time:3.33s\n",
      "Epoch 12/40, loss:0.2009, val_loss:0.2001, time:3.37s\n",
      "Epoch 13/40, loss:0.2024, val_loss:0.2006, time:3.31s\n",
      "Epoch 14/40, loss:0.2039, val_loss:0.2043, time:3.27s\n",
      "Epoch 15/40, loss:0.2031, val_loss:0.1999, time:3.34s\n",
      "Epoch 16/40, loss:0.2018, val_loss:0.1991, time:3.31s\n",
      "Epoch 17/40, loss:0.2003, val_loss:0.1999, time:3.32s\n",
      "Epoch 18/40, loss:0.1999, val_loss:0.2012, time:3.39s\n",
      "Epoch 19/40, loss:0.2010, val_loss:0.2019, time:3.24s\n",
      "Epoch 20/40, loss:0.2028, val_loss:0.2001, time:3.26s\n",
      "Epoch 21/40, loss:0.2016, val_loss:0.1999, time:3.24s\n",
      "Epoch 22/40, loss:0.2000, val_loss:0.1988, time:3.37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40, loss:0.1991, val_loss:0.1993, time:3.28s\n",
      "Epoch 24/40, loss:0.1999, val_loss:0.1996, time:3.30s\n",
      "Epoch 25/40, loss:0.2006, val_loss:0.1998, time:3.31s\n",
      "Epoch 26/40, loss:0.2008, val_loss:0.2000, time:3.27s\n",
      "Epoch 27/40, loss:0.2002, val_loss:0.2001, time:3.26s\n",
      "Epoch 28/40, loss:0.1988, val_loss:0.1989, time:3.34s\n",
      "Epoch 29/40, loss:0.1989, val_loss:0.1991, time:3.26s\n",
      "Epoch 30/40, loss:0.1991, val_loss:0.1988, time:3.28s\n",
      "Epoch 31/40, loss:0.1998, val_loss:0.1996, time:3.29s\n",
      "Epoch 32/40, loss:0.1999, val_loss:0.1990, time:3.36s\n",
      "Epoch 33/40, loss:0.1991, val_loss:0.1990, time:3.31s\n",
      "Epoch 34/40, loss:0.1985, val_loss:0.2014, time:3.28s\n",
      "Epoch 35/40, loss:0.1986, val_loss:0.1990, time:3.33s\n",
      "Epoch 36/40, loss:0.1990, val_loss:0.1988, time:3.30s\n",
      "Epoch 37/40, loss:0.1994, val_loss:0.1988, time:3.30s\n",
      "Epoch 38/40, loss:0.1989, val_loss:0.2000, time:3.33s\n",
      "Epoch 39/40, loss:0.1987, val_loss:0.1984, time:3.29s\n",
      "Epoch 40/40, loss:0.1981, val_loss:0.1985, time:3.34s\n",
      "===== Fold 9 =====\n",
      "Epoch 1/40, loss:0.2340, val_loss:0.2107, time:3.34s\n",
      "Epoch 2/40, loss:0.2139, val_loss:0.2411, time:3.29s\n",
      "Epoch 3/40, loss:0.2136, val_loss:0.2396, time:3.28s\n",
      "Epoch 4/40, loss:0.2095, val_loss:0.2049, time:3.33s\n",
      "Epoch 5/40, loss:0.2049, val_loss:0.2028, time:3.33s\n",
      "Epoch 6/40, loss:0.2025, val_loss:0.2040, time:3.30s\n",
      "Epoch 7/40, loss:0.2032, val_loss:0.2022, time:3.36s\n",
      "Epoch 8/40, loss:0.2046, val_loss:0.2167, time:3.26s\n",
      "Epoch 9/40, loss:0.2053, val_loss:0.2068, time:3.26s\n",
      "Epoch 10/40, loss:0.2036, val_loss:0.2103, time:3.29s\n",
      "Epoch 11/40, loss:0.2013, val_loss:0.2022, time:3.33s\n",
      "Epoch 12/40, loss:0.2003, val_loss:0.2088, time:3.31s\n",
      "Epoch 13/40, loss:0.2014, val_loss:0.2084, time:3.31s\n",
      "Epoch 14/40, loss:0.2031, val_loss:0.2224, time:3.34s\n",
      "Epoch 15/40, loss:0.2031, val_loss:0.2141, time:3.28s\n",
      "Epoch 16/40, loss:0.2018, val_loss:0.2013, time:3.28s\n",
      "Epoch 17/40, loss:0.1996, val_loss:0.2012, time:3.36s\n",
      "Epoch 18/40, loss:0.1999, val_loss:0.2066, time:3.30s\n",
      "Epoch 19/40, loss:0.2009, val_loss:0.2021, time:3.30s\n",
      "Epoch 20/40, loss:0.2013, val_loss:0.2010, time:3.33s\n",
      "Epoch 21/40, loss:0.2005, val_loss:0.2015, time:3.37s\n",
      "Epoch 22/40, loss:0.1995, val_loss:0.2019, time:3.28s\n",
      "Epoch 23/40, loss:0.1987, val_loss:0.2012, time:3.27s\n",
      "Epoch 24/40, loss:0.1990, val_loss:0.2011, time:3.32s\n",
      "Epoch 25/40, loss:0.2003, val_loss:0.2022, time:3.31s\n",
      "Epoch 26/40, loss:0.2004, val_loss:0.2018, time:3.29s\n",
      "Epoch 27/40, loss:0.1995, val_loss:0.2034, time:3.32s\n",
      "Epoch 28/40, loss:0.1985, val_loss:0.2017, time:3.39s\n",
      "Epoch 29/40, loss:0.1981, val_loss:0.2047, time:3.27s\n",
      "Epoch 30/40, loss:0.1990, val_loss:0.2026, time:3.33s\n",
      "Epoch 31/40, loss:0.1992, val_loss:0.2027, time:3.35s\n",
      "Epoch 32/40, loss:0.1992, val_loss:0.2015, time:3.27s\n",
      "Epoch 33/40, loss:0.1985, val_loss:0.2037, time:3.28s\n",
      "Epoch 34/40, loss:0.1977, val_loss:0.2011, time:3.30s\n",
      "Epoch 35/40, loss:0.1982, val_loss:0.2025, time:3.29s\n",
      "Epoch 36/40, loss:0.1988, val_loss:0.2029, time:3.32s\n",
      "Epoch 37/40, loss:0.1986, val_loss:0.2022, time:3.30s\n",
      "Epoch 38/40, loss:0.1989, val_loss:0.2048, time:3.31s\n",
      "Epoch 39/40, loss:0.1978, val_loss:0.2016, time:3.30s\n",
      "Epoch 40/40, loss:0.1975, val_loss:0.2011, time:3.28s\n",
      "===== Fold 10 =====\n",
      "Epoch 1/40, loss:0.2412, val_loss:0.2210, time:3.30s\n",
      "Epoch 2/40, loss:0.2199, val_loss:0.2142, time:3.28s\n",
      "Epoch 3/40, loss:0.2176, val_loss:0.2118, time:3.32s\n",
      "Epoch 4/40, loss:0.2117, val_loss:0.2129, time:3.26s\n",
      "Epoch 5/40, loss:0.2070, val_loss:0.2052, time:3.32s\n",
      "Epoch 6/40, loss:0.2039, val_loss:0.2058, time:3.28s\n",
      "Epoch 7/40, loss:0.2036, val_loss:0.2133, time:3.33s\n",
      "Epoch 8/40, loss:0.2063, val_loss:0.2049, time:3.29s\n",
      "Epoch 9/40, loss:0.2048, val_loss:0.2087, time:3.28s\n",
      "Epoch 10/40, loss:0.2039, val_loss:0.2099, time:3.35s\n",
      "Epoch 11/40, loss:0.2016, val_loss:0.2030, time:3.24s\n",
      "Epoch 12/40, loss:0.2005, val_loss:0.2038, time:3.28s\n",
      "Epoch 13/40, loss:0.2014, val_loss:0.2053, time:3.32s\n",
      "Epoch 14/40, loss:0.2031, val_loss:0.2083, time:3.26s\n",
      "Epoch 15/40, loss:0.2027, val_loss:0.2047, time:3.27s\n",
      "Epoch 16/40, loss:0.2014, val_loss:0.2068, time:3.32s\n",
      "Epoch 17/40, loss:0.1998, val_loss:0.2043, time:3.37s\n",
      "Epoch 18/40, loss:0.1996, val_loss:0.2033, time:3.29s\n",
      "Epoch 19/40, loss:0.2009, val_loss:0.2079, time:3.28s\n",
      "Epoch 20/40, loss:0.2020, val_loss:0.2064, time:3.35s\n",
      "Epoch 21/40, loss:0.2012, val_loss:0.2040, time:3.32s\n",
      "Epoch 22/40, loss:0.1999, val_loss:0.2033, time:3.27s\n",
      "Epoch 23/40, loss:0.1989, val_loss:0.2078, time:3.37s\n",
      "Epoch 24/40, loss:0.1995, val_loss:0.2040, time:3.28s\n",
      "Epoch 25/40, loss:0.1998, val_loss:0.2037, time:3.27s\n",
      "Epoch 26/40, loss:0.2010, val_loss:0.2051, time:3.29s\n",
      "Epoch 27/40, loss:0.1997, val_loss:0.2090, time:3.36s\n",
      "Epoch 28/40, loss:0.1987, val_loss:0.2043, time:3.30s\n",
      "Epoch 29/40, loss:0.1983, val_loss:0.2032, time:3.27s\n",
      "Epoch 30/40, loss:0.1989, val_loss:0.2038, time:3.34s\n",
      "Epoch 31/40, loss:0.2000, val_loss:0.2034, time:3.31s\n",
      "Epoch 32/40, loss:0.1991, val_loss:0.2032, time:3.30s\n",
      "Epoch 33/40, loss:0.1986, val_loss:0.2036, time:3.33s\n",
      "Epoch 34/40, loss:0.1980, val_loss:0.2026, time:3.27s\n",
      "Epoch 35/40, loss:0.1980, val_loss:0.2029, time:3.27s\n",
      "Epoch 36/40, loss:0.1986, val_loss:0.2040, time:3.28s\n",
      "Epoch 37/40, loss:0.1994, val_loss:0.2031, time:3.39s\n",
      "Epoch 38/40, loss:0.1985, val_loss:0.2034, time:3.31s\n",
      "Epoch 39/40, loss:0.1983, val_loss:0.2027, time:3.34s\n",
      "Epoch 40/40, loss:0.1977, val_loss:0.2027, time:3.36s\n",
      "===== Fold 11 =====\n",
      "Epoch 1/40, loss:0.2342, val_loss:0.2152, time:3.30s\n",
      "Epoch 2/40, loss:0.2146, val_loss:0.2119, time:3.33s\n",
      "Epoch 3/40, loss:0.2109, val_loss:0.2086, time:3.31s\n",
      "Epoch 4/40, loss:0.2067, val_loss:0.2088, time:3.28s\n",
      "Epoch 5/40, loss:0.2038, val_loss:0.2057, time:3.26s\n",
      "Epoch 6/40, loss:0.2019, val_loss:0.2101, time:3.37s\n",
      "Epoch 7/40, loss:0.2035, val_loss:0.2079, time:3.30s\n",
      "Epoch 8/40, loss:0.2048, val_loss:0.2097, time:3.30s\n",
      "Epoch 9/40, loss:0.2051, val_loss:0.2080, time:3.36s\n",
      "Epoch 10/40, loss:0.2029, val_loss:0.2131, time:3.27s\n",
      "Epoch 11/40, loss:0.2015, val_loss:0.2073, time:3.31s\n",
      "Epoch 12/40, loss:0.2000, val_loss:0.2059, time:3.33s\n",
      "Epoch 13/40, loss:0.2020, val_loss:0.2072, time:3.29s\n",
      "Epoch 14/40, loss:0.2025, val_loss:0.2108, time:3.29s\n",
      "Epoch 15/40, loss:0.2020, val_loss:0.2072, time:3.33s\n",
      "Epoch 16/40, loss:0.2010, val_loss:0.2055, time:3.34s\n",
      "Epoch 17/40, loss:0.1993, val_loss:0.2053, time:3.27s\n",
      "Epoch 18/40, loss:0.1994, val_loss:0.2099, time:3.29s\n",
      "Epoch 19/40, loss:0.2005, val_loss:0.2081, time:3.36s\n",
      "Epoch 20/40, loss:0.2013, val_loss:0.2108, time:3.30s\n",
      "Epoch 21/40, loss:0.2008, val_loss:0.2079, time:3.29s\n",
      "Epoch 22/40, loss:0.1993, val_loss:0.2057, time:3.34s\n",
      "Epoch 23/40, loss:0.1985, val_loss:0.2055, time:3.30s\n",
      "Epoch 24/40, loss:0.1993, val_loss:0.2054, time:3.33s\n",
      "Epoch 25/40, loss:0.1997, val_loss:0.2103, time:3.28s\n",
      "Epoch 26/40, loss:0.2003, val_loss:0.2062, time:3.33s\n",
      "Epoch 27/40, loss:0.1993, val_loss:0.2064, time:3.32s\n",
      "Epoch 28/40, loss:0.1982, val_loss:0.2062, time:3.33s\n",
      "Epoch 29/40, loss:0.1983, val_loss:0.2074, time:3.31s\n",
      "Epoch 30/40, loss:0.1988, val_loss:0.2065, time:3.29s\n",
      "Epoch 31/40, loss:0.1990, val_loss:0.2073, time:3.27s\n",
      "Epoch 32/40, loss:0.1992, val_loss:0.2083, time:3.32s\n",
      "Epoch 33/40, loss:0.1983, val_loss:0.2059, time:3.26s\n",
      "Epoch 34/40, loss:0.1978, val_loss:0.2070, time:3.28s\n",
      "Epoch 35/40, loss:0.1978, val_loss:0.2060, time:3.30s\n",
      "Epoch 36/40, loss:0.1984, val_loss:0.2071, time:3.35s\n",
      "Epoch 37/40, loss:0.1985, val_loss:0.2082, time:3.29s\n"
     ]
    }
   ],
   "source": [
    "for fold_, (train_idx, val_idx) in enumerate(splits):\n",
    "    \n",
    "    x_train_fold = torch.tensor(train_features[train_idx], dtype=torch.float).cuda()\n",
    "    y_train_fold = torch.tensor(train_target[train_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    x_val_fold = torch.tensor(train_features[val_idx], dtype=torch.float).cuda()\n",
    "    y_val_fold = torch.tensor(train_target[val_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "\n",
    "    # Loss function \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Build model, initial weight and optimizer    \n",
    "    model = SimpleNN(200, 16)\n",
    "    model.cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    \n",
    "    # Cycling learning rate\n",
    "    step_size = 2000\n",
    "    base_lr, max_lr = 0.001, 0.005\n",
    "    optimizer = optim.Adam(filter(lambda p:p.requires_grad, model.parameters()), lr=max_lr)\n",
    "    scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, \n",
    "                         step_size=step_size, mode='exp_range', gamma=0.99994)\n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print('===== Fold {} ====='.format(fold_ + 1))\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            y_pred = model(x_batch)\n",
    "            \n",
    "            if scheduler:\n",
    "                scheduler.batch_step()\n",
    "                \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "        model.eval()\n",
    "        valid_pred_folds = np.zeros((x_val_fold.size(0)))    \n",
    "        \n",
    "        avg_val_loss = 0. \n",
    "        # avg_val_auc = 0.\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            y_val_pred = model(x_batch).detach()\n",
    "            # avg_val_auc += round(roc_auc_score(y_batch.cpu(), \n",
    "            #       sigmoid(y_pred.cpu().numpy())[:, 0]),4) / len(valid_loader)\n",
    "            avg_val_loss += loss_fn(y_val_pred, y_batch).item() / len(valid_loader)\n",
    "            valid_pred_folds[i * batch_size:(i + 1) * batch_size] = sigmoid(y_val_pred.cpu().numpy())[:, 0]\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print('Epoch {}/{}, loss:{:.4f}, val_loss:{:.4f}, time:{:.2f}s'.format(\n",
    "            epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n",
    "        \n",
    "    total_train_loss.append(avg_loss)\n",
    "    total_val_loss.append(avg_val_loss)\n",
    "    \n",
    "    test_pred_folds = np.zeros((len(test_features)))\n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "        test_pred_folds[i * batch_size:(i + 1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "    pred_train[val_idx] = valid_pred_folds    \n",
    "    pred_test += test_pred_folds / len(splits)\n",
    "        \n",
    "cv_score = round(roc_auc_score(train_target, pred_train), 4)      \n",
    "print('## K-fold, Train Loss:{:.4f}, Valid Loss:{:.4f}, CV score:{:.4f}'.format(\n",
    "    np.average(total_train_loss), np.average(total_val_loss), cv_score))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## K-fold, Train Loss:0.1980, Valid Loss:0.2049, CV score:0.8963\n"
     ]
    }
   ],
   "source": [
    "cv_score = round(roc_auc_score(train_target, pred_train), 4)      \n",
    "print('## K-fold, Train Loss:{:.4f}, Valid Loss:{:.4f}, CV score:{:.4f}'.format(\n",
    "    np.average(total_train_loss), np.average(total_val_loss), cv_score))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'nn_submission.csv'\n",
    "submission_df = test_df[['ID_code']]\n",
    "submission_df['target'] = pred_test\n",
    "submission_df.to_csv(file_name, header=True, index=False, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
